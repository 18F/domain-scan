#!/usr/bin/env python3

import os
import sys
import glob
from scanners import utils
import datetime
import logging
import importlib
import shutil
import csv
from concurrent.futures import ThreadPoolExecutor

# basic setup - logs, output dirs
options = utils.options()
domain_suffix = options.get("suffix")
utils.configure_logging(options)
utils.mkdir_p(utils.cache_dir())
utils.mkdir_p(utils.results_dir())

# some metadata about the scan itself
start_time = utils.utc_timestamp()
start_command = str.join(" ", sys.argv)

###
# Entry point. `options` is a dict of CLI flags.
###
def run(options=None):

    if not options["_"]:
        logging.error("Provide a CSV file, or domain name.")
        exit()

    if not options.get("scan"):
        logging.error("--scan must be one or more scanners.")
        exit()

    domains = options["_"][0]

    # Which scanners to run the domain through.
    scans = []

    for name in options.get("scan").split(","):
        try:
            scanner = importlib.import_module("scanners.%s" % name)
        except ImportError:
            logging.error("[%s] Scanner not found, or had an error during loading." % name)
            exit()

        # If the scanner has a canonical command, make sure it exists.
        if hasattr(scanner, "command") and scanner.command and (not utils.try_command(scanner.command)):
            logging.error("[%s] Command not found: %s" %
                          (name, scanner.command))
            exit()

        # Scanners can have an optional init/validation hook.
        if hasattr(scanner, "init") and scanner.init and (not scanner.init(options)):
            logging.error("[%s] Scanner's init hook returned false." % name)
            exit()

        scans.append(scanner)

    scan_domains(scans, domains)


###
# Given the selected scanners, and input domains, run each domain
# through each scanner.
#
# Produces a CSV for each scan, with each domain and results.
###
def scan_domains(scanners, domains):

    # Clear out existing result CSVs, to avoid inconsistent data.
    for result in glob.glob("%s/*.csv" % utils.results_dir()):
        os.remove(result)

    # Run through each scanner and open a file and CSV for each.
    handles = {}
    for scanner in scanners:
        name = scanner.__name__.split(".")[-1]  # e.g. 'inspect'
        scanner_filename = "%s/%s.csv" % (utils.results_dir(), name)
        scanner_file = open(scanner_filename, 'w', newline='')
        scanner_writer = csv.writer(scanner_file)
        scanner_writer.writerow(["Domain"] + scanner.headers)

        handles[scanner] = {
            'file': scanner_file,
            'filename': scanner_filename,
            'writer': scanner_writer
        }

    # The task wrapper that's parallelized using executor.map.
    def process_scan(params):
        scanner, domain, options = params
        # A scanner can return multiple rows.

        for row in scanner.scan(domain, options):
            if row:
                handles[scanner]['writer'].writerow([domain] + row)


    # Run each scanner (unique process pool) over each domain.
    # User can force --serial, and scanners can override default of 10.
    for scanner in scanners:

        if options.get("serial"):
            workers = 1
        elif hasattr(scanner, "workers"):
            workers = scanner.workers
        else:
            workers = int(options.get("workers", 10))

        with ThreadPoolExecutor(max_workers=workers) as executor:
            tasks = ((scanner, domain, options) for domain in domains_from(domains))
            executor.map(process_scan, tasks)

    # Close up all the files, --sort if requested (expensive).
    for scanner in scanners:
        handles[scanner]['file'].close()
        if options.get("sort"):
            sort_csv(handles[scanner]['filename'])

    logging.warn("Results written to CSV.")

    # Save metadata.
    metadata = {
        'start_time': start_time,
        'end_time': utils.utc_timestamp(),
        'command': start_command
    }
    utils.write(utils.json_for(metadata), "%s/meta.json" % utils.results_dir())



# Yield domain names from a single string, or a CSV of them.
def domains_from(arg):
    if arg.endswith(".csv"):
        with open(arg, encoding='utf-8', newline='') as csvfile:
            for row in csv.reader(csvfile):
                if (not row[0]) or (row[0].lower().startswith("domain")):
                    continue
                domain = row[0].lower()
                if domain_suffix:
                    yield "%s.%s" % (domain, domain_suffix)
                else:
                    yield domain
    else:
        yield arg

# Sort a CSV by domain name, "in-place" (by making a temporary copy).
# This loads the whole thing into memory: it's not a great solution for
# super-large lists of domains.
def sort_csv(input_filename):
    logging.warn("Sorting %s..." % input_filename)

    input_file = open(input_filename, encoding='utf-8', newline='')
    tmp_filename = "%s.tmp" % input_filename
    tmp_file = open(tmp_filename, 'w', newline='')
    tmp_writer = csv.writer(tmp_file)

    # store list of domains, to sort at the end
    domains = []

    # index rows by domain
    rows = {}
    header = None

    for row in csv.reader(input_file):
        # keep the header around
        if (row[0].lower().startswith("domain")):
            header = row
            continue

        # index domain for later reference
        domain = row[0]
        domains.append(domain)
        rows[domain] = row

    # straight alphabet sort
    domains.sort()

    # write out to a new file
    tmp_writer.writerow(header)
    for domain in domains:
        tmp_writer.writerow(rows[domain])

    # close the file handles
    input_file.close()
    tmp_file.close()

    # replace the original
    shutil.move(tmp_filename, input_filename)

if __name__ == '__main__':
    run(options)
